{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "from utils import plot_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmm import GaussianMixture\n",
    "from sklearn import mixture\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=20,\n",
       "                means_init=None, n_components=3, n_init=1, precisions_init=None,\n",
       "                random_state=0, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "                verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
    "                                                    iris.target, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 0,\n",
    "                                                    stratify = iris.target)\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "sk_gmm = mixture.GaussianMixture(n_components=3,\n",
    "                                    covariance_type='full', \n",
    "                                    max_iter=20, \n",
    "                                    random_state=0)\n",
    "sk_gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk train acc: 0.9107142857142857\n",
      "sk test acc: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "print('sk train acc:', accuracy_score(y_true=y_train, y_pred=sk_gmm.predict(X_train)))\n",
    "print('sk test acc:', accuracy_score(y_true=y_test, y_pred=sk_gmm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different that supervised learning, for unsupervised learning algorithm like GMM. You just fit it with `X_train` and tell it how many clusters you want. After the training, it can predict both old and new data with a descent accuracy. Now let's see what's the magic behind this learning algorithm. As usual, we start with the inference.\n",
    "\n",
    "To predict `X`, we first need to calculate the joint log likelihood of `X`. Assuming that our training data are drawn from `n_components` Gaussian distributions, and we need to know the `mean_`, the `covariance_` of each Gaussian distribution and their relative `weights_`. If we have these parameters, we can calculate ll as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as MVN\n",
    "def _ll_joint(X, n_components, means_, covariances_, weights_):\n",
    "    ll = np.empty((n_components, X.shape[0]))\n",
    "    for k in range(n_components):\n",
    "        mvn = MVN(mean=means_[k], cov=covariances_[k])\n",
    "        ll[k] = mvn.logpdf(x=X) + np.log(weights_[k])\n",
    "    return ll.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `MVN` refers a multivariate normal distribution. So our goal is, given `n_components` and training data, learn `means_, covariances_, weights_` so we can predict new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, n_components, means_, covariances_, weights_):\n",
    "    ll = _ll_joint(X, n_components, means_, covariances_, weights_)\n",
    "    return np.argmax(ll, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize these parameters.\n",
    "\n",
    "We can randomly initialize these parameters and update them during training. But random initialize is not a good idea in unsupervised learning. It is considered best practice to initialize these parameters with `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "n_components = 3\n",
    "resp = np.zeros((m, n_components))\n",
    "kmeans = KMeans(n_clusters=n_components, random_state=0)\n",
    "labels = kmeans.fit(X_train).labels_\n",
    "resp[np.arange(m), labels] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get `labels` from `KMeans` and using one hot encoding assign the label to a matrix called `resp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `X` and `resp` to initialize `weights_`, `means_` and `covariances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_covariances(resp, X, total_weights_, means_):\n",
    "    \"\"\"\n",
    "    covariances_: shape(n_components, n_feature, n_feature)\n",
    "    \"\"\"\n",
    "    covariances_ = np.empty((n_components, n, n))\n",
    "    for k in range(n_components):\n",
    "        diff = X - means_[k]\n",
    "        covariances_[k] = np.dot(resp[:, k] * diff.T, diff) / total_weights_[k]\n",
    "    return covariances_\n",
    "\n",
    "def _M_step(X, resp):\n",
    "    \"\"\"\n",
    "    means_: shape(n_components, n_features)\n",
    "    \"\"\"\n",
    "    total_weights_ = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means_ = np.dot(resp.T, X) / total_weights_[:, np.newaxis]\n",
    "    covariances_ = _update_covariances(resp, X, total_weights_, means_)\n",
    "    weights_ = total_weights_ / m\n",
    "    return means_, covariances_, weights_\n",
    "means_, covariances_, weights_ = _M_step(X_train, resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `total_weights_` are calculated by suming up the column vectors of `resp` as the number of data points in each component.\n",
    "- Then we use dot product of `resp.T` and `X` to divided by the `total_weights_` to get the `means_` of each component.\n",
    "- We update each compoents variance by theory.\n",
    "- And finally we calculate the average `weights_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def _E_step(X, n_components, means_, covariances_, weights_):\n",
    "    ll = _ll_joint(X, n_components, means_, covariances_, weights_)\n",
    "    resp = softmax(ll, axis=1)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "for it in range(max_iter):\n",
    "    # E-step\n",
    "    resp = _E_step(X_train, n_components, means_, covariances_, weights_)\n",
    "\n",
    "    # M-step\n",
    "    means_, covariances_, weights_ = _M_step(X_train, resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = predict(X_train, n_components, means_, covariances_, weights_)\n",
    "test_preds = predict(X_test, n_components, means_, covariances_, weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107142857142857"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true=y_train, y_pred=train_preds)\n",
    "accuracy_score(y_true=y_test, y_pred=test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
