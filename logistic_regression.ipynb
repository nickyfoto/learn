{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Logistic Regression step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've see the theory part of Logistic Regression, now let's see how to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from evaluation import test\n",
    "from utils import load_data, predict_image, scatter_plot, contour_plot\n",
    "from utils import plot_boundary, load_cat_dataset, load_iris_2D\n",
    "from utils import costs_plot\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lr import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "clf = linear_model.LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `X`, we need to learn weight so that our model can predict results using\n",
    "\n",
    "```python\n",
    "sigmoid(np.dot(X, coef_) + intercept_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n_features = X.shape\n",
    "coef_ = np.zeros(shape=(1, n_features))\n",
    "intercept_ = np.zeros(shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every iteration we update the weights by the vectorized equation given by theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.51392593, 0.35277722]]), array([-4.99600361e-18]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape = (m, 1)\n",
    "max_iter = 100\n",
    "learning_rate = 1e-2\n",
    "for step in range(max_iter):  \n",
    "    preds = sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "    error = preds - y\n",
    "    gradient = np.dot(X.T, error) \n",
    "    coef_ -= learning_rate * gradient.T / m\n",
    "    intercept_ -= learning_rate * error.sum() / m\n",
    "coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the learning, we can use the weight and the sigmoid function to predict new data. If the output value is greater than 0.5, we output 1, otherwise we output 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29594077],\n",
       "       [0.20090799],\n",
       "       [0.70405923],\n",
       "       [0.79909201]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "sigmoid(np.dot(X, coef_.T) + intercept_).round().astype(np.int).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that for now our algorithm only support binary classification with value `[0,1]`. What if we have other labels values such as `[1,2]` or even text ones? [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) comes to rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Multiclass Dataset\n",
    "\n",
    "`LabelEncoder` removes the restriction our label value, but our model is still a binary classifier. We need to make some change to our algorithm so that it can predict more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [1, 1], \n",
    "              [2, 1],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "y = np.array([1, 1, 2, 2, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the above dataset. The first thing we need to do is to change the shape of `coef_` and `intercept_` so that when we use \n",
    "\n",
    "```python\n",
    "np.dot(X, coef_.T) + intercept_\n",
    "```\n",
    "\n",
    "to predict the label, it can output a row vector of the probability that item belongs to each of the three class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "m, n_features = X.shape\n",
    "coef_ = np.zeros(shape=(n_classes, n_features))\n",
    "intercept_ = np.zeros(shape=(n_classes,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two method to predict multiclass labels, we look at one versus rest first. On `ovr` setting, we format our multiclass task into `k` binary classification problem, where `k` refers to the number of classes in the training set. In the above example, we have `k = 3`. For every binary classification, we need to change `y` into\n",
    "\n",
    "```python\n",
    "y_i = np.apply_along_axis(lambda x: np.where(x == i, 1, 0), axis=0, arr=y)\n",
    "```\n",
    "\n",
    "Then we training and save the learning weights into `coef_` and `intercept_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we training and save the learning weights into `coef_` and `intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32043175, -1.03902171],\n",
       "       [ 0.95449385, -1.02779511],\n",
       "       [ 0.03911706,  0.67693788]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01246303, -0.28576712, -1.72829777])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "classes_ = le.classes_\n",
    "k = len(classes_)\n",
    "max_iter = 1000\n",
    "for i in range(k):\n",
    "    y_i = np.apply_along_axis(lambda x: np.where(x == i, 1, 0), axis=0, arr=y)\n",
    "    for step in range(max_iter):  \n",
    "        preds = sigmoid(np.dot(X, coef_[i].T) + intercept_[i])\n",
    "        error = preds - y_i\n",
    "        gradient = np.dot(X.T, error) \n",
    "        coef_[i] -= learning_rate * gradient.T / m\n",
    "        intercept_[i] -= learning_rate * error.sum() / m\n",
    "coef_\n",
    "intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we use our sigmoid function to predict new data. But instead of outputing a single value to threshold at 0.5, it outputs the probability of a example belongs to each class, we classify the class with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.12694714e-01, 4.47082450e-01, 7.98525066e-02],\n",
       "       [9.75093928e-01, 2.37404969e-01, 7.70251656e-02],\n",
       "       [8.53394251e-02, 4.11185102e-01, 2.66541155e-01],\n",
       "       [2.43078387e-02, 6.44609029e-01, 2.74257909e-01],\n",
       "       [2.62940044e-06, 1.57086393e-01, 9.26144159e-01],\n",
       "       [2.34672728e-08, 1.38635322e-01, 9.81312291e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "scores\n",
    "classes_[scores.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap up all the above step in a `LogisticRegression` class, we can train and predict as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [1, 1], \n",
    "              [2, 1],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "y = np.array([1, 1, 2, 2, 3, 3])\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "preds = clf.predict(X)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finishes our first implementation of logistic regression. Note that our model learns weights in a iterative manner called gradient descent. For each epoch, we averge the gradient and update the weights once. Another alternative gradient descent method, you might have heard is called stochastic gradient descent, which update the weight on every training example. Let's see how to implement it next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you do after implementing logistic regression? Of course we can do binary classification! It means we can linearly separate a dataset if it has two classes. For example, the iris dataset, if we combine label 1 and 2 as one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(linear_model.SGDClassifier)\n",
    "print(linear_model.SGDClassifier.decision_function.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [1, 1], \n",
    "              [2, 1],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "Y = np.array([1, 1, 2, 2, 3, 3])\n",
    "Y = np.array([1, 1, 2, 2, 2, 2])\n",
    "# Y = np.array(['a', 'a', 'b', 'b', 'b', 'b'])\n",
    "# from sklearn.utils import check_X_y\n",
    "# print(check_X_y(X, Y, 'csr', dtype=np.float64, order=\"C\",\n",
    "#                          accept_large_sparse=False))\n",
    "clf = linear_model.SGDClassifier(max_iter=1000, tol=None, loss='log')\n",
    "clf.fit(X, Y)\n",
    "clf.coef_.shape, clf.coef_\n",
    "clf.intercept_.shape, clf.intercept_\n",
    "clf.predict(X)\n",
    "# clf.predict_proba(X)\n",
    "print(clf.predict.__doc__)\n",
    "clf.average\n",
    "clf.loss_function_.__doc__\n",
    "print(clf._fit_multiclass.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute the validation split using the multiclass labels\n",
    "\n",
    "clf = LogisticRegression(print_cost = False)\n",
    "clf.fit(X, Y)\n",
    "clf.coef_.shape, clf.intercept_.shape\n",
    "clf.weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_fit_binary(est, y, i):\n",
    "    # also prepares when est.classes_ == 2\n",
    "    y_i = np.ones(y.shape, dtype=np.float64, order=\"C\")\n",
    "    y_i[y != est.classes_[i]] = -1.0\n",
    "    average_intercept = 0\n",
    "    average_coef = None\n",
    "    coef = est.coef_[i]\n",
    "    intercept = est.intercept_[i]\n",
    "    return y_i, coef, intercept, average_coef, average_intercept\n",
    "\n",
    "def fit_binary(est, i, X, y, alpha, C, learning_rate, max_iter,\n",
    "               pos_weight, neg_weight, sample_weight, validation_mask=None,\n",
    "               random_state=None):\n",
    "    y_i, coef, intercept, average_coef, average_intercept = \\\n",
    "        _prepare_fit_binary(est, y, i)\n",
    "    assert y_i.shape[0] == y.shape[0] == sample_weight.shape[0]\n",
    "    result = plain_sgd(coef, intercept, est.loss_function_,\n",
    "                           penalty_type, alpha, C, est.l1_ratio,\n",
    "                           dataset, validation_mask, est.early_stopping,\n",
    "                           validation_score_cb, int(est.n_iter_no_change),\n",
    "                           max_iter, tol, int(est.fit_intercept),\n",
    "                           int(est.verbose), int(est.shuffle), seed,\n",
    "                           pos_weight, neg_weight,\n",
    "                           learning_rate_type, est.eta0,\n",
    "                           est.power_t, est.t_, intercept_decay)\n",
    "    \n",
    "    \n",
    "def _make_validation_split(self, y):\n",
    "        \"\"\"Split the dataset between training set and validation set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples, )\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        validation_mask : array, shape (n_samples, )\n",
    "            Equal to 1 on the validation set, 0 on the training set.\n",
    "        \"\"\"\n",
    "        n_samples = y.shape[0]\n",
    "        validation_mask = np.zeros(n_samples, dtype=np.uint8)\n",
    "        if not self.early_stopping:\n",
    "            # use the full set for training, with an empty validation set\n",
    "            return validation_mask\n",
    "\n",
    "        if is_classifier(self):\n",
    "            splitter_type = StratifiedShuffleSplit\n",
    "        else:\n",
    "            splitter_type = ShuffleSplit\n",
    "        cv = splitter_type(test_size=self.validation_fraction,\n",
    "                           random_state=self.random_state)\n",
    "        idx_train, idx_val = next(cv.split(np.zeros(shape=(y.shape[0], 1)), y))\n",
    "        if idx_train.shape[0] == 0 or idx_val.shape[0] == 0:\n",
    "            raise ValueError(\n",
    "                \"Splitting %d samples into a train set and a validation set \"\n",
    "                \"with validation_fraction=%r led to an empty set (%d and %d \"\n",
    "                \"samples). Please either change validation_fraction, increase \"\n",
    "                \"number of samples, or disable early_stopping.\"\n",
    "                % (n_samples, self.validation_fraction, idx_train.shape[0],\n",
    "                   idx_val.shape[0]))\n",
    "\n",
    "        validation_mask[idx_val] = 1\n",
    "        return validation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris_2D()\n",
    "# pd.DataFrame(X).describe()\n",
    "# pd.DataFrame(X).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot the data in two dimension\n",
    "scatter_plot(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skclf = linear_model.LogisticRegression(fit_intercept=True, solver='lbfgs')\n",
    "skclf.fit(X,y)\n",
    "contour_plot(X, y, skclf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do if our data is not linearly separatable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microchip = load_data('microchip_tests.txt')\n",
    "X = microchip.iloc[:,:2].values\n",
    "y = microchip.iloc[:,2].values\n",
    "scatter_plot(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some feature engineering trick to increase the number of features of our original data so that it can be linearly seperatable in high dimension. Then visualize how curve, which is also the line in hign dimensional space to seperate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=7)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skclf = linear_model.LogisticRegression(solver='newton-cg')\n",
    "skclf.fit(X_poly, y)\n",
    "scatter_plot(X, y)\n",
    "plot_boundary(skclf, X, y, grid_step=.01, poly_featurizer=poly);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more interesting example would be to use logistic regression to build a classifier for cat images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, num_px, classes = load_cat_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skclf = linear_model.LogisticRegression(penalty='none', solver='lbfgs', max_iter=1000)\n",
    "skclf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image = \"my_image3.jpg\"   # change this to the name of your image file \n",
    "predict_image(clf= skclf, fname=cat_image, num_px=num_px, classes=classes, plot_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cat_image = \"my_image.jpg\"\n",
    "predict_image(clf= skclf, fname=not_cat_image, num_px=num_px, classes=classes, plot_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough motivation examples! Next step let's see how we can build our own logistic regression from scratch.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf = linear_model.LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                                      max_iter=200,\n",
    "                                      multi_class='multinomial').fit(X, y)\n",
    "clf.score(X, y)\n",
    "clf = linear_model.LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                                      max_iter=200,\n",
    "                                      multi_class='ovr').fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://github.com/beckernick/logistic_regression_from_scratch\n",
    "\n",
    "https://github.com/martinpella/logistic-reg/blob/master/logistic_reg.ipynb\n",
    "\n",
    "https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-3-regularization\n",
    "\n",
    "https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python/blob/master/LogisticRegression/ML_RegularizedLogisticRegression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(num_iterations = 2000, \n",
    "                         steps = 20,\n",
    "                         learning_rate = 0.005,\n",
    "                         print_cost = False)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_plot(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
