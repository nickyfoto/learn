{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Logistic Regression step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've see the theory part of Logistic Regression, now let's see how to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from evaluation import test\n",
    "from utils import load_data, predict_image, scatter_plot, contour_plot\n",
    "from utils import plot_boundary, load_cat_dataset, load_iris_2D\n",
    "from utils import costs_plot\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lr import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "clf = linear_model.LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `X`, we need to learn weight so that our model can predict results using\n",
    "\n",
    "```python\n",
    "sigmoid(np.dot(X, coef_) + intercept_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n_features = X.shape\n",
    "coef_ = np.zeros(shape=(1, n_features))\n",
    "intercept_ = np.zeros(shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every iteration we update the weights by the vectorized equation given by theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.51392593, 0.35277722]]), array([-4.99600361e-18]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape = (m, 1)\n",
    "max_iter = 100\n",
    "learning_rate = 1e-2\n",
    "for step in range(max_iter):  \n",
    "    preds = sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "    error = preds - y\n",
    "    gradient = np.dot(X.T, error) \n",
    "    coef_ -= learning_rate * gradient.T / m\n",
    "    intercept_ -= learning_rate * error.sum() / m\n",
    "coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the learning, we can use the weight and the sigmoid function to predict new data. If the output value is greater than 0.5, we output 1, otherwise we output 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29594077],\n",
       "       [0.20090799],\n",
       "       [0.70405923],\n",
       "       [0.79909201]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "sigmoid(np.dot(X, coef_.T) + intercept_).round().astype(np.int).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that for now our algorithm only support binary classification with value `[0,1]`. What if we have other labels values such as `[1,2]` or even text ones? [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) comes to rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Multiclass Dataset\n",
    "\n",
    "`LabelEncoder` removes the restriction our label value, but our model is still a binary classifier. We need to make some change to our algorithm so that it can predict more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [1, 1], \n",
    "              [2, 1],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "y = np.array([1, 1, 2, 2, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the above dataset. The first thing we need to do is to change the shape of `coef_` and `intercept_` so that when we use \n",
    "\n",
    "```python\n",
    "np.dot(X, coef_.T) + intercept_\n",
    "```\n",
    "\n",
    "to predict the label, it can output a row vector of the probability that item belongs to each of the three class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y))\n",
    "m, n_features = X.shape\n",
    "coef_ = np.zeros(shape=(n_classes, n_features))\n",
    "intercept_ = np.zeros(shape=(n_classes,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two method to predict multiclass labels, we look at one versus rest first. On `ovr` setting, we format our multiclass task into `k` binary classification problem, where `k` refers to the number of classes in the training set. In the above example, we have `k = 3`. For every binary classification, we need to change `y` into\n",
    "\n",
    "```python\n",
    "y_i = np.apply_along_axis(lambda x: np.where(x == i, 1, 0), axis=0, arr=y)\n",
    "```\n",
    "\n",
    "Then we training and save the learning weights into `coef_` and `intercept_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we training and save the learning weights into `coef_` and `intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32043175, -1.03902171],\n",
       "       [ 0.95449385, -1.02779511],\n",
       "       [ 0.03911706,  0.67693788]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01246303, -0.28576712, -1.72829777])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "classes_ = le.classes_\n",
    "k = len(classes_)\n",
    "max_iter = 1000\n",
    "for i in range(k):\n",
    "    y_i = np.apply_along_axis(lambda x: np.where(x == i, 1, 0), axis=0, arr=y)\n",
    "    for step in range(max_iter):  \n",
    "        preds = sigmoid(np.dot(X, coef_[i].T) + intercept_[i])\n",
    "        error = preds - y_i\n",
    "        gradient = np.dot(X.T, error) \n",
    "        coef_[i] -= learning_rate * gradient.T / m\n",
    "        intercept_[i] -= learning_rate * error.sum() / m\n",
    "coef_\n",
    "intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we use our sigmoid function to predict new data. But instead of outputing a single value to threshold at 0.5, it outputs the probability of a example belongs to each class, we classify the class with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.12694714e-01, 4.47082450e-01, 7.98525066e-02],\n",
       "       [9.75093928e-01, 2.37404969e-01, 7.70251656e-02],\n",
       "       [8.53394251e-02, 4.11185102e-01, 2.66541155e-01],\n",
       "       [2.43078387e-02, 6.44609029e-01, 2.74257909e-01],\n",
       "       [2.62940044e-06, 1.57086393e-01, 9.26144159e-01],\n",
       "       [2.34672728e-08, 1.38635322e-01, 9.81312291e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "scores\n",
    "classes_[scores.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap up all the above step in a `LogisticRegression` class, we can train and predict as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [1, 1], \n",
    "              [2, 1],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "y = np.array([1, 1, 2, 2, 3, 3])\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "preds = clf.predict(X)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finishes our first implementation of logistic regression. Note that our model learns weights in a iterative manner called gradient descent. For each epoch, we averge the gradient and update the weights once. Another alternative gradient descent method, you might have heard is called stochastic gradient descent, which update the weight on every training example. Let's see how to implement it next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous we use `X` to calculate the predictions for the whole training set.\n",
    "\n",
    "```python\n",
    "preds = sigmoid(np.dot(X, self.coef_.T) + self.intercept_)\n",
    "```\n",
    "\n",
    "Now for every training example `x`, we predict it as:\n",
    "\n",
    "```python\n",
    "pred = sigmoid(np.dot(x, self.coef_.T) + self.intercept_)\n",
    "```\n",
    "\n",
    "Then update `coef_` and `intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.09231379, 0.78699242]]), array([-0.00190078]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "m, n_features = X.shape\n",
    "coef_ = np.zeros(shape=(1, n_features))\n",
    "intercept_ = np.zeros(shape=(1,))\n",
    "y.shape = (m, 1)\n",
    "max_iter = 100\n",
    "learning_rate = 1e-2\n",
    "for step in range(max_iter):\n",
    "    for idx, x in enumerate(X):\n",
    "        pred = sigmoid(np.dot(x, coef_.T) + intercept_)\n",
    "        error = pred - y[idx]\n",
    "        gradient = x * error\n",
    "        coef_ -= learning_rate * gradient.T\n",
    "        intercept_ -= learning_rate * error\n",
    "coef_, intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction part remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1322503 ],\n",
       "       [0.04863655],\n",
       "       [0.86731283],\n",
       "       [0.95118724]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, coef_.T) + intercept_)\n",
    "sigmoid(np.dot(X, coef_.T) + intercept_).round().astype(np.int).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap all the details in class we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=True, learning_rate=0.01, max_iter=100,\n",
       "                   print_cost=False, sgd=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "clf = LogisticRegression(sgd = True, max_iter=100, learning_rate = 1e-2)\n",
    "clf.fit(X, y)\n",
    "clf.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
