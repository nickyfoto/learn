{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes, Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen the [theory part](https://nickyfoto.github.io/blog/entries/naive-bayes) of Naive Bayes, now let's see how to implement it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from learn.naive_bayes import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randint(2, size=(4, 4))\n",
    "y = np.array([1, 0, 1, 0])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "X\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X$, we want to calculate the probability that $x$ belong to each class and classify $x$ with the highest value. We need feature probability and prior class probability to carry out the calculation?\n",
    "\n",
    "The class probability is equal to the number of training examples in each category divided by the number of training examples. We can use the [`LabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to binarizer our label to facilitate \n",
    "further computation.\n",
    "\n",
    "```python\n",
    "labelbin = LabelBinarizer()\n",
    "Y = labelbin.fit_transform(y)\n",
    "```\n",
    "\n",
    "If y is a binary classification problem as our above example, we need to add one extra column to make Y a two column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelbin = LabelBinarizer()\n",
    "Y = labelbin.fit_transform(y)\n",
    "if Y.shape[1] == 1:\n",
    "    Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation, the class count and feature count can simply be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 1],\n",
       "       [1, 2, 2, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count_ = Y.sum(axis=0)\n",
    "feature_count_ = np.dot(Y.T, X)\n",
    "class_count_\n",
    "feature_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability distribution can be simply calculated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.4, 0.2, 0.2],\n",
       "       [0.2, 0.4, 0.4, 0. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count_/class_count_.sum()\n",
    "feature_count_ / feature_count_.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, as with many machine learning algorihtm, we use log probability to calculate the distribution in order to avoid the [underflow error](https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01) caused by multiplying many probability. So the above can be written as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.69314718, -0.69314718])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.60943791, -0.91629073, -1.60943791, -1.60943791],\n",
       "       [-1.60943791, -0.91629073, -0.91629073,        -inf]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_log_prior_ = np.log (class_count_) - np.log(class_count_.sum())\n",
    "feature_log_prob_ = np.log(feature_count_) - np.log(feature_count_.sum(axis=1)).reshape(-1, 1)\n",
    "class_log_prior_\n",
    "feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have a divide by zero error because one class of training example have zero sum on that feature. Here comes the Laplace smoothing technique. Before we divide the sum of features, we add 1 to each feature count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.48490665, -2.07944154, -2.48490665, -2.48490665],\n",
       "       [-2.19722458, -1.79175947, -1.79175947, -2.89037176]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_log_prob_ = np.log(feature_count_ +1) - np.log(feature_count_ +1).sum(axis=1).reshape(-1, 1)\n",
    "feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the training steps are our model is trying to learn. Let's see how we predict new data based on the what we learned so far. Given a dataset X, we calculate the joint log likelihood of each training example with our `feature_log_prob` and `class_log_prior`. Dot product help use evaluate the joint likelihood of a single training example on different classes. We also add the `class_log_prior_` as information of the distribution of classes in the training dataset. And finally we use `argmax` from numpy to output the class label we saved on `labelbin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-7.74240202, -6.4738907 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2:3]\n",
    "jll = np.dot(X[2:3], feature_log_prob_.T) + class_log_prior_\n",
    "jll\n",
    "labelbin.classes_[np.argmax(jll, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiveBayes().fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Sex_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.  , 180.  ,  12.  ],\n",
       "       [  5.92, 190.  ,  11.  ],\n",
       "       [  5.58, 170.  ,  12.  ],\n",
       "       [  5.92, 165.  ,  10.  ],\n",
       "       [  5.  , 100.  ,   6.  ],\n",
       "       [  5.5 , 150.  ,   8.  ],\n",
       "       [  5.42, 130.  ,   7.  ],\n",
       "       [  5.75, 150.  ,   9.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[6, 180, 12],\n",
    "              [5.92, 190, 11],\n",
    "              [5.58, 170, 12],\n",
    "              [5.92, 165, 10],\n",
    "              [5, 100, 6],\n",
    "              [5.5, 150, 8],\n",
    "              [5.42, 130, 7],\n",
    "              [5.75, 150, 9]\n",
    "             ])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male 1, female 0\n",
    "y = np.array([1,1,1,1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.  , 180.  ,  12.  ],\n",
       "       [  5.92, 190.  ,  11.  ],\n",
       "       [  5.58, 170.  ,  12.  ],\n",
       "       [  5.92, 165.  ,  10.  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  5.855, 176.25 ,  11.25 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3.50333333e-02, 1.22916667e+02, 9.16666667e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male = X[y == 1]\n",
    "male\n",
    "male.mean(axis=0)\n",
    "male.var(axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.  , 100.  ,   6.  ],\n",
       "       [  5.5 , 150.  ,   8.  ],\n",
       "       [  5.42, 130.  ,   7.  ],\n",
       "       [  5.75, 150.  ,   9.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  5.4175, 132.5   ,   7.5   ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([9.72250000e-02, 5.58333333e+02, 1.66666667e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female = X[y == 0]\n",
    "female\n",
    "female.mean(axis=0)\n",
    "female.var(axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6., 130.,   8.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[6, 130, 8.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = NaiveBayes().fit(X, y)\n",
    "clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55248494, 0.44751506]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.197071843878072e-09"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "mvn(mean=male.mean(axis=0), cov=male.var(axis=0, ddof=1)).pdf(x)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005377909183630023"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvn(mean=female.mean(axis=0), cov=female.var(axis=0, ddof=1)).pdf(x)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
