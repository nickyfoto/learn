{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from evaluation import test\n",
    "from utils import load_data, predict_image, scatter_plot, contour_plot\n",
    "from utils import plot_boundary, load_cat_dataset, load_iris_2D\n",
    "from utils import costs_plot\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lr import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you do after implementing logistic regression? Of course we can do binary classification! It means we can linearly separate a dataset if it has two classes. For example, the iris dataset, if we combine label 1 and 2 as one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(linear_model.SGDClassifier)\n",
    "print(linear_model.SGDClassifier.decision_function.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], \n",
    "              [-2, -1], \n",
    "              [1, 1], \n",
    "              [2, 1],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "Y = np.array([1, 1, 2, 2, 3, 3])\n",
    "Y = np.array([1, 1, 2, 2, 2, 2])\n",
    "# Y = np.array(['a', 'a', 'b', 'b', 'b', 'b'])\n",
    "# from sklearn.utils import check_X_y\n",
    "# print(check_X_y(X, Y, 'csr', dtype=np.float64, order=\"C\",\n",
    "#                          accept_large_sparse=False))\n",
    "clf = linear_model.SGDClassifier(max_iter=1000, tol=None, loss='log')\n",
    "clf.fit(X, Y)\n",
    "clf.coef_.shape, clf.coef_\n",
    "clf.intercept_.shape, clf.intercept_\n",
    "clf.predict(X)\n",
    "# clf.predict_proba(X)\n",
    "print(clf.predict.__doc__)\n",
    "clf.average\n",
    "clf.loss_function_.__doc__\n",
    "print(clf._fit_multiclass.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute the validation split using the multiclass labels\n",
    "\n",
    "clf = LogisticRegression(print_cost = False)\n",
    "clf.fit(X, Y)\n",
    "clf.coef_.shape, clf.intercept_.shape\n",
    "clf.weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_fit_binary(est, y, i):\n",
    "    # also prepares when est.classes_ == 2\n",
    "    y_i = np.ones(y.shape, dtype=np.float64, order=\"C\")\n",
    "    y_i[y != est.classes_[i]] = -1.0\n",
    "    average_intercept = 0\n",
    "    average_coef = None\n",
    "    coef = est.coef_[i]\n",
    "    intercept = est.intercept_[i]\n",
    "    return y_i, coef, intercept, average_coef, average_intercept\n",
    "\n",
    "def fit_binary(est, i, X, y, alpha, C, learning_rate, max_iter,\n",
    "               pos_weight, neg_weight, sample_weight, validation_mask=None,\n",
    "               random_state=None):\n",
    "    y_i, coef, intercept, average_coef, average_intercept = \\\n",
    "        _prepare_fit_binary(est, y, i)\n",
    "    assert y_i.shape[0] == y.shape[0] == sample_weight.shape[0]\n",
    "    result = plain_sgd(coef, intercept, est.loss_function_,\n",
    "                           penalty_type, alpha, C, est.l1_ratio,\n",
    "                           dataset, validation_mask, est.early_stopping,\n",
    "                           validation_score_cb, int(est.n_iter_no_change),\n",
    "                           max_iter, tol, int(est.fit_intercept),\n",
    "                           int(est.verbose), int(est.shuffle), seed,\n",
    "                           pos_weight, neg_weight,\n",
    "                           learning_rate_type, est.eta0,\n",
    "                           est.power_t, est.t_, intercept_decay)\n",
    "    \n",
    "    \n",
    "def _make_validation_split(self, y):\n",
    "        \"\"\"Split the dataset between training set and validation set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples, )\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        validation_mask : array, shape (n_samples, )\n",
    "            Equal to 1 on the validation set, 0 on the training set.\n",
    "        \"\"\"\n",
    "        n_samples = y.shape[0]\n",
    "        validation_mask = np.zeros(n_samples, dtype=np.uint8)\n",
    "        if not self.early_stopping:\n",
    "            # use the full set for training, with an empty validation set\n",
    "            return validation_mask\n",
    "\n",
    "        if is_classifier(self):\n",
    "            splitter_type = StratifiedShuffleSplit\n",
    "        else:\n",
    "            splitter_type = ShuffleSplit\n",
    "        cv = splitter_type(test_size=self.validation_fraction,\n",
    "                           random_state=self.random_state)\n",
    "        idx_train, idx_val = next(cv.split(np.zeros(shape=(y.shape[0], 1)), y))\n",
    "        if idx_train.shape[0] == 0 or idx_val.shape[0] == 0:\n",
    "            raise ValueError(\n",
    "                \"Splitting %d samples into a train set and a validation set \"\n",
    "                \"with validation_fraction=%r led to an empty set (%d and %d \"\n",
    "                \"samples). Please either change validation_fraction, increase \"\n",
    "                \"number of samples, or disable early_stopping.\"\n",
    "                % (n_samples, self.validation_fraction, idx_train.shape[0],\n",
    "                   idx_val.shape[0]))\n",
    "\n",
    "        validation_mask[idx_val] = 1\n",
    "        return validation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris_2D()\n",
    "# pd.DataFrame(X).describe()\n",
    "# pd.DataFrame(X).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot the data in two dimension\n",
    "scatter_plot(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skclf = linear_model.LogisticRegression(fit_intercept=True, solver='lbfgs')\n",
    "skclf.fit(X,y)\n",
    "contour_plot(X, y, skclf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we do if our data is not linearly separatable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microchip = load_data('microchip_tests.txt')\n",
    "X = microchip.iloc[:,:2].values\n",
    "y = microchip.iloc[:,2].values\n",
    "scatter_plot(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some feature engineering trick to increase the number of features of our original data so that it can be linearly seperatable in high dimension. Then visualize how curve, which is also the line in hign dimensional space to seperate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=7)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skclf = linear_model.LogisticRegression(solver='newton-cg')\n",
    "skclf.fit(X_poly, y)\n",
    "scatter_plot(X, y)\n",
    "plot_boundary(skclf, X, y, grid_step=.01, poly_featurizer=poly);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more interesting example would be to use logistic regression to build a classifier for cat images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, num_px, classes = load_cat_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skclf = linear_model.LogisticRegression(penalty='none', solver='lbfgs', max_iter=1000)\n",
    "skclf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image = \"my_image3.jpg\"   # change this to the name of your image file \n",
    "predict_image(clf= skclf, fname=cat_image, num_px=num_px, classes=classes, plot_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cat_image = \"my_image.jpg\"\n",
    "predict_image(clf= skclf, fname=not_cat_image, num_px=num_px, classes=classes, plot_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough motivation examples! Next step let's see how we can build our own logistic regression from scratch.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "clf = linear_model.LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                                      max_iter=200,\n",
    "                                      multi_class='multinomial').fit(X, y)\n",
    "clf.score(X, y)\n",
    "clf = linear_model.LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                                      max_iter=200,\n",
    "                                      multi_class='ovr').fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://github.com/beckernick/logistic_regression_from_scratch\n",
    "\n",
    "https://github.com/martinpella/logistic-reg/blob/master/logistic_reg.ipynb\n",
    "\n",
    "https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-3-regularization\n",
    "\n",
    "https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python/blob/master/LogisticRegression/ML_RegularizedLogisticRegression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(num_iterations = 2000, \n",
    "                         steps = 20,\n",
    "                         learning_rate = 0.005,\n",
    "                         print_cost = False)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_plot(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
