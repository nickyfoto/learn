{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from evaluation import test\n",
    "from utils import load_data, predict_image, scatter_plot, contour_plot\n",
    "from utils import plot_boundary, load_cat_dataset, load_iris_2D\n",
    "from utils import plot_costs\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# from lr import LogisticRegression\n",
    "from lm import LinearRegression, Ridge, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_polynomial_feats(x, degree):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: numpy array of length N, the 1-D observations\n",
    "        degree: the max polynomial degree\n",
    "    Return:\n",
    "        feat: numpy array of shape Nx(degree+1), remember to include \n",
    "        the bias term. feat is in the format of:\n",
    "        [[1.0, x1, x1^2, x1^3, ....,],\n",
    "         [1.0, x2, x2^2, x2^3, ....,],\n",
    "         ......\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # raise NotImplementedError\n",
    "    n = x.shape[0]\n",
    "    res = np.empty((degree+1,n))\n",
    "    for i in range(degree+1):\n",
    "        res[i] = np.power(x,i)\n",
    "    \n",
    "    return res.T\n",
    "# Here there are two data points. One data point is 0.5 and other one is 0.2. \n",
    "# We try to represent these two data points in higher dimensions.\n",
    "\n",
    "#helper do not need to change\n",
    "def plot_curve(x, y, curve_type='.', color='b', lw=2):\n",
    "    plt.plot(x, y, curve_type, color=color, linewidth=lw)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(True)\n",
    "    \n",
    "def rmse(pred, label): \n",
    "    '''\n",
    "    This is the root mean square error.\n",
    "    Args:\n",
    "        pred: numpy array of length N * 1, the prediction of labels\n",
    "        label: numpy array of length N * 1, the ground truth of labels\n",
    "    Return:\n",
    "        a float value\n",
    "    '''\n",
    "    #raise NotImplementedError\n",
    "    N = pred.shape \n",
    "    return np.sqrt(((label - pred) ** 2).sum()/N)[0]\n",
    "# Hint: get the sqaure root of theta hat in slide 14 (https://mahdi-roozbahani.github.io/cse4240-spring2019.github.io/course/14-linear-regression.pdf)\n",
    "prediction = np.array([1, 2, 3]) \n",
    "label = np.array([1.5, 2.5, 3.5])\n",
    "print('rmse =',rmse(prediction, label))\n",
    "\n",
    "    \n",
    "x = np.array([0.5, 0.2]) \n",
    "x_feat = construct_polynomial_feats(x, 4)\n",
    "print(x_feat)\n",
    "#helper, do not need to change\n",
    "\n",
    "POLY_DEGREE = 5\n",
    "NUM_OBS = 1000\n",
    "\n",
    "rng = np.random.RandomState(seed=4)\n",
    "\n",
    "true_weight = rng.rand(POLY_DEGREE + 1, 1)\n",
    "true_weight[2:, :] = 0\n",
    "x_all = np.linspace(-5, 5, NUM_OBS)\n",
    "x_all_feat = construct_polynomial_feats(x_all, POLY_DEGREE)\n",
    "y_all = np.dot(x_all_feat, true_weight) + rng.randn(x_all_feat.shape[0], 1) # in the second term, we add noise to data\n",
    "# Note that here we try to produce y_all as our training data\n",
    "plot_curve(x_all, y_all) # Data with noise that we are going to predict\n",
    "plot_curve(x_all, np.dot(x_all_feat, true_weight), curve_type='-', color='r', lw=4) # the groundtruth information\n",
    "\n",
    "indices = rng.permutation(NUM_OBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(object):\n",
    "    @staticmethod \n",
    "    # static method means that you can use this method or function for any other classes, it is not specific to LinearReg\n",
    "    def fit_closed(xtrain, ytrain):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytarin: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        weight = np.linalg.inv(xtrain.T.dot(xtrain)).dot(xtrain.T).dot(ytrain)\n",
    "        return weight\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit_GD(xtrain, ytrain, epochs=100, learning_rate=0.001,plot_loss=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        N, D = xtrain.shape\n",
    "        weight = np.zeros((D,1))\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            prediction = np.dot(xtrain, weight)\n",
    "            error = prediction - ytrain\n",
    "            weight = weight - (learning_rate * (1/N) * np.dot(xtrain.T, error))\n",
    "            if plot_loss:\n",
    "                cost = rmse((xtrain.dot(weight)), ytrain)\n",
    "                costs.append(cost)\n",
    "        if plot_loss:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set(xlabel=\"# Epochs\", ylabel=\"RMSE per Epoch\")\n",
    "            ax.plot(range(epochs), costs)\n",
    "        return weight\n",
    "    @staticmethod\n",
    "    def fit_SGD(xtrain, ytrain, epochs=100, learning_rate=0.001, plot_loss=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        N, D = xtrain.shape\n",
    "        weight = np.zeros((D,1))\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            #indices = np.arange(N)\n",
    "            #np.random.shuffle(indices)\n",
    "            #xtrain = xtrain[indices]\n",
    "            #ytrain = ytrain[indices]\n",
    "            for j in range(N):\n",
    "                deriv = (xtrain[j].dot(weight) - ytrain[j]) * xtrain[[j]]\n",
    "                #print(deriv.shape, xtrain[j].shape)\n",
    "                weight -= learning_rate*deriv.T\n",
    "            if plot_loss:\n",
    "                preds = np.dot(xtrain, weight)\n",
    "                cost = rmse(preds, ytrain)\n",
    "                costs.append(cost)\n",
    "        if plot_loss:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.set(xlabel=\"# Epochs\", ylabel=\"RMSE per Epoch\")\n",
    "            ax.plot(range(epochs), costs)\n",
    "        return weight\n",
    "    @staticmethod\n",
    "    def predict(xtest, weight):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtest: NxD numpy array, where N is number \n",
    "                   of instances and D is the dimensionality of each \n",
    "                   instance\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        Return:\n",
    "            prediction: Nx1 numpy array, the predicted labels\n",
    "        \"\"\"\n",
    "        #raise NotImplementedError\n",
    "        #print(xtest.shape, weight.shape, weight.T.shape)\n",
    "        return (xtest.dot(weight))\n",
    "# Hint: in the fit function, use close form solution of the linear regression to get weights. \n",
    "# For inverse, you can use numpy linear algebra function  \n",
    "# For the predict, you need to use linear combination of data points and their weights (y = w0*1+w1*X1+...)\n",
    "#helper, do not need to change\n",
    "\n",
    "train_indices = indices[:NUM_OBS//2]\n",
    "test_indices = indices[NUM_OBS//2:]\n",
    "\n",
    "plot_curve(x_all[train_indices], y_all[train_indices], color='r')\n",
    "plot_curve(x_all[test_indices], y_all[test_indices], color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper, do not need to change\n",
    "\n",
    "weight = LinearReg.fit_closed(x_all_feat[train_indices], y_all[train_indices])\n",
    "y_test_pred = LinearReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = LinearReg.fit_GD(x_all_feat[train_indices], y_all[train_indices], epochs=500000, learning_rate=1e-7,\n",
    "                          plot_loss=True)\n",
    "y_test_pred = LinearReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = LinearReg.fit_SGD(x_all_feat[train_indices], y_all[train_indices], epochs=2500, learning_rate=1e-7,\n",
    "                           plot_loss=True)\n",
    "y_test_pred = LinearReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = LinearReg.fit_closed(x_all_feat[sub_train], y_all[sub_train])\n",
    "y_test_pred = LinearReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "np.sqrt(mse(y_true=y_all[test_indices], y_pred=y_test_pred))\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=False).fit(x_all_feat[sub_train], y_all[sub_train])\n",
    "np.sqrt(mse(y_true=y_all[test_indices], \n",
    "            y_pred=reg.predict(x_all_feat[test_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper, do not need to change\n",
    "\n",
    "y_pred = LinearReg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred, curve_type='-', color='b', lw=4)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeReg(LinearReg):\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_closed(xtrain, ytrain, c_lambda):\n",
    "        N, D = xtrain.shape\n",
    "        L = np.eye(D)\n",
    "        L[0,0] = 0\n",
    "        weight = np.linalg.inv(xtrain.T.dot(xtrain) + c_lambda * L).dot(xtrain.T).dot(ytrain)\n",
    "        return weight\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit_GD(xtrain, ytrain, c_lambda, epochs=100, learning_rate=0.001,plot_loss=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        N, D = xtrain.shape\n",
    "        weight = np.zeros((D,1))\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            preds = np.dot(xtrain, weight)\n",
    "            error = preds - ytrain\n",
    "\n",
    "            gradient = np.dot(xtrain.T, error)\n",
    "            weight[0] -= learning_rate * gradient[0] / N\n",
    "            weight[1:] -= learning_rate * (gradient[1:] +  c_lambda * weight[1:])/N\n",
    "#             weight[1:] = weight[1:] * (1- learning_rate * c_lambda/N)- (learning_rate * (1/N) * gradient[1:])\n",
    "\n",
    "            cost = rmse(preds, ytrain)\n",
    "            costs.append(cost)\n",
    "        if plot_loss:\n",
    "            fig, ax = plt.subplots()\n",
    "            costs = costs[:100]\n",
    "            ax.plot(range(len(costs)), costs)\n",
    "        return weight\n",
    "    @staticmethod\n",
    "    def fit_SGD(xtrain, ytrain, c_lambda, epochs=100, learning_rate=0.001,plot_loss=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        N, D = xtrain.shape\n",
    "        weight = np.zeros((D,1))\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            for j in range(N):\n",
    "                prediction = np.dot(xtrain[j], weight)\n",
    "                error = prediction - ytrain[j]\n",
    "                gradient = (xtrain[[j]] * error).T\n",
    "                weight[0] -= learning_rate * gradient[0]\n",
    "                weight[1:] -= learning_rate * (gradient[1:] +  c_lambda * weight[1:]/N)\n",
    "            if plot_loss:\n",
    "                preds = np.dot(xtrain, weight)\n",
    "                cost = rmse(preds, ytrain)\n",
    "                costs.append(cost)\n",
    "        if plot_loss:\n",
    "            fig, ax = plt.subplots()\n",
    "            costs = costs[:100]\n",
    "            ax.plot(range(len(costs)), costs)\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper, do not need to change\n",
    "\n",
    "weight = RidgeReg.fit_closed(x_all_feat[train_indices], y_all[train_indices],  c_lambda=1)\n",
    "y_test_pred = RidgeReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)\n",
    "reg_r = linear_model.Ridge(fit_intercept=False).fit(x_all_feat[train_indices], y_all[train_indices])\n",
    "np.sqrt(mse(y_true=y_all[test_indices], \n",
    "            y_pred=reg_r.predict(x_all_feat[test_indices])))\n",
    "ridge_cv = linear_model.RidgeCV(alphas=[1e-3,1e-2,1e-1, 1,1e2, 1e3, 1e4], \n",
    "                                fit_intercept=False).fit(x_all_feat[train_indices], y_all[train_indices])\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = linear_model.RidgeCV(alphas=[1e-3,1e-2,1e-1, 1,1e2, 1e3, 1e4,1e5,1e6,1e7], \n",
    "                                fit_intercept=False).fit(x_all_feat[sub_train], y_all[sub_train])\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = RidgeReg.fit_closed(x_all_feat[sub_train], y_all[sub_train], c_lambda=1000)\n",
    "\n",
    "y_pred = RidgeReg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = RidgeReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)\n",
    "\n",
    "\n",
    "reg_r = linear_model.Ridge(alpha=100000, fit_intercept=False).fit(x_all_feat[sub_train], y_all[sub_train])\n",
    "np.sqrt(mse(y_true=y_all[test_indices], \n",
    "            y_pred=reg_r.predict(x_all_feat[test_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = RidgeReg.fit_SGD(x_all_feat[sub_train], y_all[sub_train], c_lambda=1000, learning_rate=1e-7)\n",
    "\n",
    "y_pred = RidgeReg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = RidgeReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse) #1.6672\n",
    "\n",
    "sgd_r = linear_model.SGDRegressor(fit_intercept=False, shuffle=False,\n",
    "                                  verbose=1,\n",
    "                                 ).fit(x_all_feat[sub_train], y_all[sub_train].flatten())\n",
    "np.sqrt(mse(y_true=y_all[test_indices], \n",
    "            y_pred=sgd_r.predict(x_all_feat[test_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = RidgeReg.fit_GD(x_all_feat[sub_train], y_all[sub_train], c_lambda=1000, learning_rate=1e-7)\n",
    "\n",
    "y_pred = RidgeReg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = RidgeReg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(diabetes_X_train, diabetes_y_train)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = linear_model.LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "lr = linear_model.LinearRegression()\n",
    "boston = datasets.load_boston()\n",
    "y = boston.target\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted = cross_val_predict(lr, boston.data, y, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "predicted = cross_val_predict(lr, boston.data, y, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading some example data\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "reg3 = linear_model.LinearRegression()\n",
    "reg3.fit(X, y)\n",
    "reg3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "reg3 = LinearRegression()\n",
    "reg3.fit(X, y)\n",
    "reg3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\" function to approximate by polynomial interpolation\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "x = np.linspace(0, 10, 100)\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(x)\n",
    "x = np.sort(x[:20])\n",
    "y = f(x)\n",
    "\n",
    "# create matrix versions of these arrays\n",
    "X = x[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"ground truth\")\n",
    "plt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate([3, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), linear_model.Ridge())\n",
    "    model.fit(X, y)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,\n",
    "             label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge()\n",
    "reg.fit(X, y)\n",
    "reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['teal', 'yellowgreen', 'gold']\n",
    "lw = 2\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"ground truth\")\n",
    "plt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate([3, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(X, y)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,\n",
    "             label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
